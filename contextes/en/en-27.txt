   #alternate Disinformation Researchers Raise Alarms About A.I. Chatbots

   Skip to contentSkip to site index
   (BUTTON) Search & Section Navigation
--
   Supported by
   SKIP ADVERTISEMENT

Disinformation Researchers Raise Alarms About A.I. Chatbots

   Researchers used ChatGPT to produce clean, convincing text that
   repeated conspiracy theories and misleading narratives.
--
   online misinformation and conducted the experiment last month.
   “Crafting a new false narrative can now be done at dramatic scale, and
   much more frequently — it’s like having A.I. agents contributing to
   disinformation.”

   Disinformation is difficult to wrangle when it’s created manually by
   humans. Researchers predict that generative technology could make
   disinformation cheaper and easier to produce for an even larger number
   of conspiracy theorists and spreaders of disinformation.

   Personalized, real-time chatbots could share conspiracy theories in
   increasingly credible and persuasive ways, researchers say, smoothing
--
   language.

   ChatGPT is far more powerful and sophisticated. Supplied with questions
   loaded with disinformation, it can produce convincing, clean variations
   on the content en masse within seconds, without disclosing its sources.
   On Tuesday, Microsoft and OpenAI introduced a new Bing search engine
   and web browser that can use chatbot technology to plan vacations,
   translate texts or conduct research.

Disinformation From ChatGPT

   When researchers at NewsGuard asked ChatGPT to write responses based on
   false and misleading ideas, the bot complied about 80 percent of the
--

   OpenAI researchers have long been nervous about chatbots falling into
   nefarious hands, writing in a 2019 paper of their “concern that its
   capabilities could lower costs of disinformation campaigns” and aid in
   the malicious pursuit “of monetary gain, a particular political agenda,
   and/or a desire to create chaos or confusion.”

--
   reference solutions to make sure I wasn’t losing my mind.”

   Researchers also worry that the technology could be exploited by
   foreign agents hoping to spread disinformation in English. Some
   companies already use multilingual chatbots to support customers
   without translators.

--
   Working last month off a sampling of 100 false narratives from before
   2022 (ChatGPT is trained mostly on data through 2021), NewsGuard asked
   the chatbot to write content advancing harmful health claims about
   vaccines, mimicking propaganda and disinformation from China and Russia
   and echoing the tone of partisan news outlets.

   The technology produced responses that seemed authoritative but were
--

Finding Its Voice

   ChatGPT was able to embody the language and voice of disinformation
   peddlers, using popular phrases like “do your own research.” In this
   example, researchers at NewsGuard asked for vaccine misinformation in
   the voice of Joseph Mercola, an anti-vaccine doctor. Pfizer updated its
--
   When The New York Times repeated the experiment using a sample of
   NewsGuard’s questions, ChatGPT was more likely to push back on the
   prompts than when researchers originally ran the test, offering
   disinformation in response to only 33 percent of the questions.
   NewsGuard said that ChatGPT was constantly changing as developers
   tweaked the algorithm and that the bot might respond differently if a
   user repeatedly inputs misinformation.
--
   Stable Diffusion image generator, which she criticized for being
   “available for anyone to use without any hard restrictions.” Stable
   Diffusion, she wrote in an open letter, can and likely has already been
   used to create “images used for disinformation and misinformation
   campaigns.”

   Check Point Research, a group providing cyber threat intelligence,
--
   Audio produced by Kate Winslett.

   Tiffany Hsu is a tech reporter covering misinformation and
   disinformation. More about Tiffany Hsu

   Stuart A. Thompson is a reporter on the Technology desk covering online
   information flows. More about Stuart A. Thompson
