   ions+2024%2CComputing%2CDeepfake] Skip to main contentSkip to
   navigationSkip to navigation

   Print subscriptions
   Sign in
   Search jobs
   Search
   Europe edition [ ]

   The Guardian - Back to homeThe Guardian

   ShowMoreShow More
       google-search
       Search










   Collage of AI-generated image of the pope wearing Balenciaga, a voting
   ballot and a person operating a smart phone.
   Illustration: Mark Harris/The Guardian
   Illustration: Mark Harris/The Guardian
   AI and the US electionUS politics
   Interview

‘An evolution in propaganda’: a digital expert on AI influence in elections

   Nick Robins-Early

   Renée DiResta of the Stanford Internet Observatory speaks about how the
   challenges of partisanship and trust are exacerbated by new
   technologies


   Thu 20 Jul 2023 16.00 CESTLast modified on Sat 28 Oct 2023 13.15 CEST

   Every election presents an opportunity for disinformation to find its
   way into the public discourse. But as the 2024 US presidential race
   begins to take shape, the growth of artificial intelligence (AI)
   technology threatens to give propagandists powerful new tools to ply
   their trade.

   Generative AI models that are able to create unique content from simple
   prompts are already being deployed for political purposes, taking
   disinformation campaigns into strange new places. Campaigns have
   circulated fake images and audio targeting other candidates, including
   an AI-generated campaign ad attacking Joe Biden and deepfake videos
   mimicking real-life news footage.
   Disinformation reimagined: how AI could erode democracy in the 2024 US
   elections
   Read more

   The Guardian spoke with Renée DiResta, technical research manager at
   the Stanford Internet Observatory, a university program that researches
   the abuses of information technology, about how the latest developments
   in AI influence campaigns and how society is catching up to a new,
   artificially created reality.

   Concern around AI and its potential for disinformation has been around
   for a while. What has changed that makes this threat more urgent?

   When people became aware of deepfakes – which usually refers to
   machine-generated video of an event that did not happen – a few years
   ago there was concern that adversarial actors would use these types of
   video to disrupt elections. Perhaps they would make video of a
   candidate, perhaps they would make video of some sort of disaster. But
   it didn’t really happen. The technology captured public attention, but
   it wasn’t very widely democratized. And so it didn’t primarily manifest
   in the political conversation, but instead in the realm of much more
   mundane but really individually harmful things, like revenge porn.

   There’s been two major developments in the last six months. First is
   the rise of ChatGPT, which is generated text. It became available to a
   mass market and people began to realize how easy it was to use these
   types of text-based tools. At the same time, text-to-still image tools
   became globally available. Today, anybody can use Stable Diffusion or
   Midjourney to create photorealistic images of things that don’t really
   exist in the world. The combination of these two things, in addition to
   the concerns that a lot of people feel around the 2024 elections, has
   really captured public attention once again.

   Why did the political use of deepfakes not materialize?

   The challenge with using video in a political environment is that you
   really have to nail the substance of the content. There are a lot of
   tells in video, a lot of ways in which you can determine whether it’s
   generated. On top of that, when a video is truly sensational, a lot of
   people look at it and factcheck it and respond to it. You might call it
   a natural immune response.

   Text and images, however, have the potential for higher actual impact
   in an election scenario because they can be more subtle and longer
   lasting. Elections require months of campaigning during which people
   formulate an opinion. It’s not something where you’re going to change
   the entire public mind with a video and have that be the most impactful
   communication of the election.

     With generative AI it is now effortless to generate highly
     personalized content and to automate its dissemination

   How do you think large language models can change political propaganda?

   I want to caveat that describing what is tactically possible is not the
   same thing as me saying the sky is falling. I’m not a doomer about this
   technology. But I do think that we should understand generative AI in
   the context of what it makes possible. It increases the number of
   people who can create political propaganda or content. It decreases the
   cost to do it. That’s not to say necessarily that they will, and so I
   think we want to maintain that differentiation between this is the
   tactic that a new technology enables versus that this is going to swing
   an election.

   As far as the question of what’s possible, in terms of behaviors,
   you’ll see things like automation. You might remember back in 2015
   there were all these fears about bots. You had a lot of people using
   automation to try to make their point of view look more popular –
   making it look like a whole lot of people think this thing, when in
   reality it’s six guys and their 5,000 bots. For a while Twitter wasn’t
   doing anything to stop that, but it was fairly easy to detect. A lot of
   the accounts would be saying the exact same thing at the exact same
   time, because it was expensive and time consuming to generate a unique
   message for each of your fake accounts. But with generative AI it is
   now effortless to generate highly personalized content and to automate
   its dissemination.

   And then finally, in terms of content, it’s really just that the
   messages are more credible and persuasive.

   That seems tied to another aspect you’ve written about, that the sheer
   amount of content that can be generated, including misleading or
   inaccurate content, has a muddying effect on information and trust.

   It’s the scale that makes it really different. People have always been
   able to create propaganda, and I think it’s very important to emphasize
   that. There is an entire industry of people whose job it is to create
   messages for campaigns and then figure out how to get them out into the
   world. We’ve just changed the speed and the scale and the cost to do
   that. It’s just an evolution in propaganda.

     Where we’ve gone with generative AI is the fabrication of a complete
     unreality, where nothing about the image is what it seems

   When we think about what’s new and what’s different here, the same
   thing goes for images. When Photoshop emerged, the public at first was
   very uncomfortable with Photoshopped images, and gradually became more
   comfortable with it. The public acclimated to the idea that Photoshop
   existed and that not everything that you see with your eyes is a thing
   that necessarily is as it seems – the idea that the woman that you see
   we’ve gone with generative AI is the fabrication of a complete
   unreality, where nothing about the image is what it seems but it looks
   photorealistic.
   skip past newsletter promotion

   Sign up to TechScape
   Free weekly newsletter

   Alex Hern's weekly dive in to how technology is shaping our lives
   Privacy Notice: Newsletters may contain info about charities, online
   ads, and content funded by outside parties. For more information see
   the Google Privacy Policy and Terms of Service apply.

   after newsletter promotion

   Now anybody can make it look like the pope is wearing Balenciaga.

   Exactly.

   In the US, it seems like meaningful federal regulation is pretty far
   away if it’s going to come at all. Absent of that, what are some of the
   sort of short-term ways to mitigate these risks?

   First is the education piece. There was a very large education
   component when deepfakes became popular – media covered them and people
   began to get the sense that we were entering a world in which a video
   might not be what it seems.

   But it’s unreasonable to expect every person engaging with somebody on
   a social media platform to figure out if the person they’re talking to
   is real. Platforms will have to take steps to more carefully identify
   if automation is in play.

   On the image front, social media platforms, as well as generative AI
   companies, are starting to come together to try and determine what kind
   determine computationally whether an image is generated.

   Some companies, like OpenAI, have policies around generating
   misinformation or the use of ChatGPT for political ends. How effective
   do you see those policies being?

   It’s a question of access. For any technology, you can try to put
   guardrails on your proprietary version of that technology and you can
   argue you’ve made a values-based decision to not allow your products to
   generate particular types of content. On the flip side, though, there
   are models that are open source and anyone can go and get access to
   them. Some of the things that are being done with some of the open
   source models and image generation are deeply harmful, but once the
   model is open sourced, the ability to control its use is much more
   limited.

   And it’s a very big debate right now in the field. You don’t want to
   necessarily create regulations that lock in and protect particular
   corporate actors. At the same time, there is a recognition that
   becomes how the platforms that are going to serve as the dissemination
   pathways for this stuff think about their role and their policies in
   what they amplify and curate.

   What’s the media or the public getting wrong about AI and
   disinformation?

   One of the real challenges is that people are going to believe what
   they see if it conforms to what they want to believe. In a world of
   unreality in which you can create that content that fulfills that need,
   solve any of the problems. Or will we move further into divergent
   realities – where people are going to continue to hold the belief in
   something that they’ve seen on the internet as long as it tells them
   what they want. Larger offline challenges around partisanship and trust
   are reflected in, and exacerbated by, new technologies that enable this
   kind of content to propagate online.
   Explore more on these topics


   Reuse this content

Most viewed

Most viewed



   Original reporting and incisive analysis, direct from the Guardian
   every morning
   Sign up for our email



   Back to top
   rights reserved. (dcr)
